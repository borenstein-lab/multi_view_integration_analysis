---
title: "Comparison of different feature selection approaches"
output:
  html_document:
    css: custom_notebook_formatting.css
    toc: true
    toc_depth: 3
    df_print: paged
    code_folding: hide
---

## Preparations

```{r PREPS, message=FALSE, warning=FALSE}
library(gridExtra) 
library(scales)
library(ggpubr)
library(rstatix)
library(dplyr)
library(stabm)
library(foreach)
library(readr)
library(multcompView)
library(cowplot)
library(knitr)

options(scipen = 100)
knitr::opts_chunk$set(cache = TRUE, cache.lazy = FALSE, results = "hold")

std_mean <- function(x) sd(x, na.rm = TRUE)/sqrt(sum(!is.na(x)))
```

Other settings:

```{r}
main_datasets <- config::get("shotgun_datasets_not_in_CMD")

fs_names <- c("none" = "No feature selection",
              "altmann" = "Altmann-based",
              "utest" = "U-test",  
              "Boruta90" = "Boruta-90", 
              'ensemble' = "Ensemble FS", 
              'pertBoruta' = "Boruta-90 with Perturbations")
fs_names2 <- c("-FS" = "No feature selection", 
               "+FS_A" = "Altmann-based",
               "+FS_U" = "U-test", 
               "+FS_B90" = "Boruta-90", 
               '+FS_ENS' = "Ensemble FS", 
               '+FS_PB90' = "Boruta-90 with Perturbations")

# Coloring
fs_colors <- c("No feature selection" = "azure3", 
               "Altmann-based" = "burlywood1",
               "U-test" = "aquamarine3",  
               "Boruta-90" = "chocolate3", 
               "Ensemble FS" = "darkgoldenrod4", 
               "Boruta-90 with Perturbations" = "darkorchid4")
feature_type_color_map = 
  c("M" = "midnightblue", 
    "T" = "firebrick4", 
    "P" = "darkorange3", 
    "G" = "chartreuse4", 
    "T+G+P" = "grey",
    "T+G+P+M" = "grey")
```


## Load data

Load data from previously organized rdata file (prepared by the `ml_pipeline/postprocess_results.R` script).

```{r LOAD_REUSLTS, results='hold', cache=TRUE}
load(config::get("paths")$results_rdata)

# Keep only selected feature sets
feature_sets <- c('T','G','P','M','T+G+P','T+G+P+M')

cv_results <- cv_results %>%
  filter(feature_set_type %in% feature_sets) %>%
  mutate(feature_set_type = 
           factor(feature_set_type, levels = feature_sets))
feat_imp <- feat_imp %>%
  filter(feature_set_type %in% feature_sets)
feat_imp_sum <- feat_imp_sum %>%
  filter(feature_set_type %in% feature_sets)

# Organize FS method names
cv_results$fs_type <- 
  factor(fs_names[cv_results$fs_type], levels = unname(fs_names))

cv_datasets_summary$fs_type <- 
  factor(fs_names[cv_datasets_summary$fs_type], levels = unname(fs_names))

feat_imp <- feat_imp %>%
  mutate(fs_type = gsub(" (T|G|P|M|T\\+G\\+P|T\\+G\\+P\\+M)$","",run_name)) %>%
  mutate(fs_type = gsub("^\\-T \\-Sh ","",fs_type)) %>%
  mutate(fs_type = factor(fs_names2[fs_type], levels = levels(cv_results$fs_type)))

cv_results_full <- cv_results # BACKUP

# Remove shuffled version
cv_results <- cv_results %>% filter(!shuffled)
feat_imp <- feat_imp %>% filter(grepl(" \\-Sh ", run_name))
feat_imp_sum <- feat_imp_sum %>% filter(grepl(" \\-Sh ", run_name))
```

## Comparison of feature selection (FS) methods

### Overview per dataset and feature set

Per dataset we show out-of-fold AUC results over different folds/repeats, for each feature selection method and each feature set. We add information about the number of features in each setting as well. The goal is to provide a high-level view of our performance and it's stability.  
The left panels describe the number of features selected using each FS method.  

* The grey line indicates the original number of features.  
* Note that the number of features presented is averaged over folds & repeats. 

The right panels describes AUC over folds and repeats. Outliers hided. 

```{r fig.height=6, fig.width=6, cache=TRUE, warning=FALSE}
run_means <- cv_results %>%
  group_by(dataset, feature_set_type, fs_type, run_name, n_features_origin) %>%
  summarise(mean_n_features_final = mean(n_features_for_train_final, na.rm = TRUE),
            se_n_features_final = std_mean(n_features_for_train_final),
            mean_auc = mean(out_of_fold_test_auc, na.rm = TRUE),
            se_auc = std_mean(out_of_fold_test_auc),
            median_auc = median(out_of_fold_test_auc, na.rm = TRUE),
            non_0_aucs = sum(!is.na(out_of_fold_test_auc)),
            N = n(), 
            .groups = "drop") 

for(curr_dataset in main_datasets) { # curr_dataset <- "cd_franzosa_2019"

  p_auc_over_folds <-
    ggplot(cv_results %>% filter(dataset == curr_dataset),
           aes(x = out_of_fold_test_auc, y = fs_type, fill = fs_type)) +
      geom_boxplot(width = 0.7, outlier.shape = NA, alpha = 0.4) +
      stat_summary(fun = "mean", shape = 4, 
                   size = 0.25, color = "grey50") +
      scale_fill_manual(values = fs_colors) +
      geom_vline(xintercept = 0.5, color = "darkred", 
                 linetype = "dashed", linewidth = 1, alpha = 0.8) +
      xlab("Out-of-fold AUC") +
      theme_bw() +
      facet_grid(feature_set_type ~ .) +
      theme(legend.position = "none") +
      theme(axis.title.y = element_blank()) +
      theme(axis.text.y = element_blank()) +
      theme(axis.text.x = element_text(size = 8)) +
      theme(axis.ticks.y = element_blank()) +
      theme(plot.margin=unit(c(5.5,5.5,5.5,0), "points"))
  
  p_n_features <- 
    ggplot(run_means %>% filter(dataset == curr_dataset),
           aes(x = fs_type, y = mean_n_features_final)) +
      geom_col(aes(fill = fs_type), color = "darkgrey", width = 0.7, alpha = 0.5) +
      geom_hline(aes(yintercept = n_features_origin), color = "grey70", alpha = 0.8, linewidth = 1.3) +
      geom_text(aes(label = round(mean_n_features_final,1)), size = 2.2, hjust = -0.3) +
      scale_y_continuous(expand = expansion(mult = c(0, 0.25))) +
      ylab("N selected features (Mean)") +
      coord_flip() +
      theme_bw() +
      scale_fill_manual(values = fs_colors) +
      facet_grid(feature_set_type ~ ., scales = "free_y") +
      theme(legend.position = "none") +
      theme(axis.title.y = element_blank()) +
      theme(axis.text = element_text(size = 8)) +
      theme(strip.background = element_blank()) +
      theme(strip.text.y = element_blank())
  
  grid.arrange(p_n_features, p_auc_over_folds, 
               nrow = 1, widths = c(0.55,0.45),
               top = paste("Dataset:", curr_dataset))
}
```

### Feature selection stability comparison

Here we compare the stability of each feature selection approach, i.e. how stable is the selection of features given slight changes in the training data. We use the Jaccard and the Somol indices as available from the "stabm" package.

```{r}
# For each dataset and feature-set, compare feature selection stability between the different methods we used
get_fs_stability <- function(curr_feature_sets, feat_imp, cv_results, datasets_to_analyze, labels = c("A","B")) {
  
  datasets_to_analyze2 <- feat_imp %>% 
    filter(dataset %in% datasets_to_analyze) %>%
    filter(feature_set_type %in% curr_feature_sets) %>%
    pull(dataset) %>%
    unique()
  
  print(paste("Calculating FS stability based on", length(datasets_to_analyze2), "datasets"))
  
  fs_stability <- 
    foreach(curr_dataset = datasets_to_analyze2,  
            .combine='bind_rows') %do% { 
      print(curr_dataset)
       
      # Extract only needed data
      tmp <- feat_imp %>% 
        filter(dataset == curr_dataset) %>%
        filter(feature_set_type %in% curr_feature_sets) %>%
        filter(fs_type != "No feature selection") %>%
        select(fs_type, fold_id, feature_set_type, feature) %>%
        filter(!is.na(feature))
      
      # Sanity
      if (n_distinct(tmp$feature_set_type) > 1) stop("Only one feature set type per dataset supported. ", curr_dataset, "has > 1")
      
      if (nrow(tmp)==0) {
        print("Feature set not available for this dataset")
        data.frame()
      }
      
      # Get toal number of features in this dataset and using the 
      #  specific feature-set, for stabm calculations
      n_features_total <- cv_results %>%
         filter(dataset == curr_dataset) %>%
         filter(feature_set_type %in% curr_feature_sets) %>%
         pull(n_features_origin) %>%
         first()
       
      # Calculate stability (using 2 different metrics) per FS method
      stab_values <- t(sapply(X = unique(as.character(tmp$fs_type)),
              FUN = function(fs) {
                # Create a list of feature sets selected in each of the 50 folds/repeats
                tmp2 <- tmp %>% filter(fs_type == fs)
                tmp2_l <- split(tmp2$feature, tmp2$fold_id)
                # Calculate and return stability using a few different metrics
                c(jaccard_value = stabilityJaccard(features = tmp2_l, p = n_features_total, correction.for.chance = "estimate", N = 20),
                  somol_value = stabilitySomol(features = tmp2_l, p = n_features_total))
              }))
       
      stab_values <- data.frame(stab_values) %>% 
         tibble::rownames_to_column(var = "fs_type") %>%
         mutate(dataset = curr_dataset) %>%
        mutate(feature_set_type = unique(tmp$feature_set_type))
      
      stab_values
  } # End of foreach loop
  
  # Now plot stability comparison
  p1 <- ggplot(fs_stability %>%
                  mutate(fs_type = factor(fs_type, levels = levels(cv_results$fs_type))), 
                aes(x = fs_type, y = jaccard_value)) +
      geom_boxplot(width = 0.7, fill = "lightgrey", alpha = 0.5, outlier.shape = NA) +
      geom_line(aes(group = paste0(dataset, feature_set_type)), 
                position = position_dodge(0.1), 
                linewidth = 0.2, color = "chocolate3", alpha = 0.5) +
      geom_point(aes(fill = fs_type, 
                     group = paste0(dataset, feature_set_type)), 
                 position = position_dodge(0.1), 
                 size = 1.2, color = "chocolate4", alpha = 0.5) +
      scale_y_continuous(limits = c(0,1)) +
      xlab(NULL) +
      ylab("Jaccard stability index\n(higher = more stable)") +
      theme_bw() +
      theme(legend.position = "none") +
      theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1)) +
      theme(axis.title = element_text(size = 12)) +
      theme(plot.title = element_text(hjust = 0.5, size = 10))
  
  p2 <- ggplot(fs_stability %>%
                  mutate(fs_type = factor(fs_type, levels = levels(cv_results$fs_type))), 
                aes(x = fs_type, y = somol_value, fill = fs_type)) +
      geom_boxplot(width = 0.7, fill = "lightgrey", alpha = 0.5, outlier.shape = NA) +
      geom_line(aes(group = paste0(dataset, feature_set_type)), 
                position = position_dodge(0.1), linewidth = 0.2, 
                color = "chocolate3", alpha = 0.5) +
      geom_point(aes(fill = fs_type, 
                     group = paste0(dataset, feature_set_type)), 
                 position = position_dodge(0.1), size = 1.2, 
                 color = "chocolate4", alpha = 0.5) +
      scale_y_continuous(limits = c(0,1)) +
      xlab(NULL) +
      ylab("Somol stability index\n(higher = more stable)") +
      theme_bw() +
      theme(legend.position = "none") +
      theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1)) +
      theme(axis.title = element_text(size = 12)) +
      theme(plot.title = element_text(hjust = 0.5, size = 10))

  print(plot_grid(p1, p2, nrow = 1, align = 'h', axis = 'tb', labels = labels))
  
  return(fs_stability)
}
```


```{r fig.width=8.5, fig.height=3.5, eval = FALSE}
tmp <- get_fs_stability(c('T'), feat_imp, cv_results, datasets_to_analyze)
tmp <- get_fs_stability(c('T+G+P+M','T+G+P'), feat_imp, cv_results, datasets_to_analyze)
rm(tmp)
```

### Tukey tests 

We perform tukey tests per dataset + feature set (e.g. taxonomy features of esrd_wang dataset), to compare classification performance using the different FS methods.

Utility functions for Tukey test. Adapted from: https://r-graph-gallery.com/84-tukey-test.html

```{r}
# Function to group together groups not significantly different according to the Tukey HSD test
generate_tukey_label_df <- function(TUKEY, tukey_variable = 'tmp$fs_type') {
 
   # Extract labels from Tukey post-hoc 
   Tukey.levels <- TUKEY[[tukey_variable]][,"p adj"]
   Letter.rev <- TUKEY[[tukey_variable]][1,"diff"] < 0
   Tukey.labels <- data.frame(
     tukey_letter = multcompLetters(Tukey.levels, 
                              threshold = 0.1, 
                              reversed = Letter.rev)$Letters
     )
   
   Tukey.labels$group <- rownames(Tukey.labels)
   rownames(Tukey.labels) <- NULL
   return(Tukey.labels)
}

tukey_boxplot_with_letters <- function(df, TUKEY, fs_levels = levels(cv_results$fs_type)) {
  # Get labels based on Tukey test
  tukey_labels <- generate_tukey_label_df(TUKEY) 
  
  # Add labels to main data frame
  df <- df %>%
    left_join(tukey_labels, by = c("fs_type" = "group")) %>%
    mutate(fs_type = gsub("_", "\\-", fs_type)) %>%
    mutate(fs_type = factor(fs_type, levels = fs_levels))
  
  # Match factor
  tukey_labels <- tukey_labels %>%
    mutate(group = gsub("_", "\\-", group)) %>%
    mutate(group = factor(group, levels = fs_levels))
  
  # A panel of colors to draw each group with the same color :
  tukey_colors <- c("a" = "#79CDCD", "ab" = "#9ACD32", "b" = "#EEAD0E", "bc" = "#CD950C", "c" = "#FF8C00")
  
  # Title
  tit <- paste(df$dataset[1], df$feature_set_type[1], sep = " ~ ")
  
  # Plot settings
  text_y <- max(df$out_of_fold_test_auc, na.rm = TRUE) + 0.01
  break_jumps <- ifelse(max(df$out_of_fold_test_auc, na.rm = TRUE) - 
                          min(df$out_of_fold_test_auc, na.rm = TRUE) > 0.2, 
                        0.1, 0.05)
  
  # Plot
  p <- ggplot(df, aes(x = fs_type, y = out_of_fold_test_auc, fill = tukey_letter)) +
    geom_boxplot() +
    stat_summary(fun = "mean", shape = 4, 
                   size = 0.25, color = "grey50") +
    scale_fill_manual(values = tukey_colors) +
    geom_text(data = tukey_labels, aes(x = group, label = tukey_letter), y = text_y) +
    scale_y_continuous(expand = c(0, 0.02), breaks = seq(0, 1, by = break_jumps)) +
    theme_bw() +
    ggtitle(tit) +
    ylab("\n\nOut of fold AUC") +
    xlab(NULL) +
    theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1)) +
    theme(plot.title = element_text(hjust = 0.5, size = 12)) +
    theme(legend.position = "none")

  return(list(p = p, tukey_labels = tukey_labels, df = df))
}
```

Plot cases where there were significant differences in performance between different FS methods.

```{r fig.width=3.3, fig.height=3, warning=FALSE}
tukey_fs <- data.frame()

# Iterate over datasets, compute Tukey per dataset
for (curr_dataset in unique(cv_results$dataset)) { # curr_dataset = "pre_ht_li_2017"
  # Extract the list of feature sets to go over
  feat_sets <- cv_results %>% 
    filter(!is.na(out_of_fold_test_auc)) %>% 
    filter(dataset == curr_dataset) %>% 
    pull(feature_set_type) %>% 
    unique() %>% 
    as.character()
  
  for (feature_set in feat_sets) { # feature_set = "T+G+P+M" 
    tmp <- cv_results %>%
      filter(dataset == curr_dataset) %>%
      filter(feature_set_type == feature_set) %>%
      # To simplify Tukey plots
      mutate(fs_type = gsub("\\-", "_", fs_type)) %>%
      select(dataset, 
             fold_id, 
             feature_set_type, 
             fs_type, 
             out_of_fold_test_auc, 
             mean_out_of_fold_test_auc)
    
    # ANOVA
    an_model <- aov(lm(tmp$out_of_fold_test_auc ~ tmp$fs_type))
     
    # Tukey HSD test
    TUKEY <- TukeyHSD(x = an_model, 'tmp$fs_type', conf.level = 0.95)
    
    # Plot only if there are significant differences
    tukey_plot <- tukey_boxplot_with_letters(tmp, TUKEY)
    if (tukey_plot$tukey_labels$tukey_letter %>% n_distinct() > 1) {
      print(tukey_plot$p)
    } else {
      print(paste(curr_dataset, "~", feature_set, ": No significant differences between FS methods"))
    }
    
    tukey_fs <- bind_rows(tukey_fs,
                          tukey_plot$tukey_labels %>% 
                            mutate(dataset = curr_dataset) %>%
                            mutate(feature_set_type = feature_set) %>%
                            rename(fs_type = group))
  }
}

rm(tmp, curr_dataset, feature_set) 
```

### Best FS per dataset/ feature-set?

Find best feature selection method per dataset and feature set.

```{r}
# Add Tukey test results 
cv_results_best_fs_tmp <- cv_results_full %>%
  left_join(tukey_fs, by = c("dataset","feature_set_type","fs_type")) %>%
  rename(tukey_letter_fs_comparison = tukey_letter)

# Take method with best mean_AUC 
#  (even if not significant according to the Tukey test)
# In tie cases - take model with less features
final_best_fs <- cv_results %>%
  filter(!shuffled) %>%
  filter(fs_type != "No feature selection") %>%
  # Keep FS methods resulting in best AUC, rounded to 3 digits. Keep ties
  group_by(dataset, feature_set_type) %>% 
  slice_max(order_by = round(mean_out_of_fold_test_auc, 3)) %>% 
  ungroup() %>%
  # Get mean number of features selected by each method
  group_by(dataset, feature_set_type, fs_type) %>% 
  mutate(mean_n_fs_selected = mean(n_features_for_train_final)) %>%
  ungroup() %>%
  # Now to break ties take method with least features selected, i.e. most simple model
  group_by(dataset, feature_set_type) %>% 
  slice_min(order_by = mean_n_fs_selected) %>% 
  ungroup() %>%
  select(dataset, feature_set_type, fs_type) %>%
  distinct()


# Filter all tables accordingly
get_best_fs_only <- function(df, best_fs = final_best_fs) {
  df %>% inner_join(best_fs, by = c("dataset", "feature_set_type", "fs_type"))
}

cv_results_best_fs <- get_best_fs_only(cv_results_best_fs_tmp)
cv_datasets_summary_best_fs <- get_best_fs_only(cv_datasets_summary)
feat_imp_best_fs <- get_best_fs_only(feat_imp)

feat_imp_sum <- feat_imp_sum %>% 
  mutate(fs_type = gsub("^.*\\-Sh ", "", gsub(" (T|G|P|M).*$", "", run_name))) %>% 
  mutate(fs_type = factor(fs_names2[fs_type], levels = levels(cv_results$fs_type))) 
feat_imp_sum_best_fs <- get_best_fs_only(feat_imp_sum)

rm(cv_results_best_fs_tmp)
```

### Datasets to discard - all models with AUC < 0.7

```{r}
datasets_to_analyze <- cv_datasets_summary_best_fs %>%
  filter(!shuffled) %>%
  group_by(dataset) %>%
  filter(0.7 <= max(mean_out_of_fold_test_auc)) %>%
  pull(dataset) %>%
  unique()
datasets_discarded <- setdiff(unique(cv_datasets_summary_best_fs$dataset), datasets_to_analyze)

message("The following datasets have AUC over 0.7: ",
          paste(datasets_to_analyze, collapse = ", "))
message("Discarded datasets: ",
          paste(datasets_discarded, collapse = ", "))
```

### Plot best vs. no FS 

Models where both AUC's (with FS and without FS) were < 0.6 are not drawn.

```{r fig.height=5, fig.width=7}
tmp1 <- cv_results %>%
  filter(dataset %in% datasets_to_analyze) %>%
  # Keep only best FS + no FS
  left_join(final_best_fs %>% mutate(keep = 1), by = c("dataset", "feature_set_type", "fs_type")) %>%
  mutate(keep = tidyr::replace_na(keep, 0)) %>%
  mutate(keep = ifelse(fs_type == "No feature selection", 1, keep)) %>%
  filter(keep == 1) %>%
  select(-keep) %>%
  filter(!is.na(out_of_fold_test_auc)) %>%
  group_by(dataset, feature_set_type, fs_type, n_features_origin) %>%
  summarise(mean_auc = mean(out_of_fold_test_auc, na.rm = TRUE),
            se_auc = std_mean(out_of_fold_test_auc),
            N = n(), 
            .groups = "drop") %>%
  mutate(FS = ifelse(fs_type == "No feature selection", "no_FS", "with_FS")) %>%
  tidyr::pivot_wider(id_cols = c(dataset, feature_set_type, n_features_origin), 
                     names_from = FS, 
                     values_from = c(mean_auc, se_auc, N)) %>%
  filter(mean_auc_no_FS > 0.6 | mean_auc_with_FS > 0.6)

tmp1$t_test_p_value <- apply(tmp1, MARGIN = 1, function(r) {
  d <- r[['dataset']]
  fst <- r[['feature_set_type']]
  aucs_with <- cv_results_best_fs %>%
    filter(dataset == d) %>%
    filter(feature_set_type == fst) %>%
    filter(!shuffled) %>%
    pull(out_of_fold_test_auc)
  aucs_without <- cv_results %>%
    filter(dataset == d) %>%
    filter(feature_set_type == fst) %>%
    filter(fs_type == "No feature selection") %>%
    filter(!shuffled) %>%
    pull(out_of_fold_test_auc)
  if(all(is.na(aucs_without))) return(1)
  return(t.test(aucs_with, aucs_without, paired = TRUE)$p.value)
})

tmp1$t_test_fdr <- p.adjust(tmp1$t_test_p_value, method = "fdr")
tmp1 <- tmp1 %>%
  mutate(significance = ifelse(t_test_fdr < 0.05, '*', ''))

ggplot(tmp1, aes(x = mean_auc_no_FS, y = mean_auc_with_FS, fill = feature_set_type)) +
  geom_abline(intercept = 0, slope = 1, linewidth = 1.5, alpha = 0.7) +
  geom_point(size = 4, alpha = 0.5, shape = 21) +
  geom_text(aes(label = significance), size = 6, vjust = 0.7) +
  theme_classic() +
  scale_fill_manual(values = feature_type_color_map, name = 'View') +
  scale_x_continuous(limits = c(0.55, 1)) +
  scale_y_continuous(limits = c(0.55, 1)) +
  xlab('Mean AUC without feature selection') +
  ylab('Mean AUC with feature selection') +
  geom_label(x = 0.65, y = 0.9, label = "Feature selection\nimproves performance", inherit.aes = FALSE, fill = 'grey100') +
  geom_label(x = 0.9, y = 0.65, label = "Feature selection does\nnot improve performance", inherit.aes = FALSE, fill = 'grey100')

message(tmp1 %>% filter(mean_auc_with_FS>mean_auc_no_FS & t_test_fdr<0.05) %>% nrow(),'/',nrow(tmp1),' models significantly improved with FS')
message(tmp1 %>% filter(mean_auc_with_FS<mean_auc_no_FS & t_test_fdr<0.05) %>% nrow(),'/',nrow(tmp1),' models significantly worsened with FS')

rm(tmp1)
```

